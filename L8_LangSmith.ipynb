{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83b49259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model calling and intial setup\n",
    "import os\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import AzureChatOpenAI , AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser \n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "load_dotenv(override= True)\n",
    "# Load env\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "AZURE_BASE_URL = os.getenv(\"AZURE_BASE_URL\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_CHAT_DEPLIOYMENT_NAME = os.getenv(\"AZURE_CHAT_DEPLIOYMENT_NAME\")\n",
    "AZURE_EMBEDDING_DEPLIOYMENT_NAME = os.getenv(\"AZURE_EMBEDDING_DEPLIOYMENT_NAME\")\n",
    "\n",
    "# get all the langsmith based env \n",
    "LANGSMITH_ENDPOINT = os.getenv(\"LANGSMITH_ENDPOINT\")\n",
    "LANGSMITH_TRACING = os.getenv(\"LANGSMITH_TRACING\")\n",
    "LANGSMITH_PROJECT = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\" , api_key= GOOGLE_API_KEY)\n",
    "\n",
    "llm_openai = AzureChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",                         \n",
    "    deployment_name=AZURE_CHAT_DEPLIOYMENT_NAME ,  # deployment name in Azure\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_BASE_URL,\n",
    "    api_version=\"2024-02-01\",\n",
    "    temperature=0.75\n",
    ") \n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(model=\"text-embedding-3-large\",\n",
    "                             deployment=AZURE_EMBEDDING_DEPLIOYMENT_NAME,\n",
    "                             api_key= AZURE_OPENAI_API_KEY,\n",
    "                             azure_endpoint= AZURE_BASE_URL,\n",
    "                             api_version=\"2024-02-01\"\n",
    "                             )\n",
    "# result = llm_openai.invoke(\"What are your creater, also what type of LLM are you\").content\n",
    "# print(result)\n",
    "# llm_gemini.invoke(\"who is father of india\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3d2724",
   "metadata": {},
   "source": [
    "### what need observablity in AI services\n",
    "- can track latency drop\n",
    "- we can log complex llm workflow\n",
    "\n",
    "### Langsmith\n",
    "- use for obersvalibilt and eval platform , where team can deugs and moniter app performace\n",
    "\n",
    "### What langsmith trace\n",
    " - i/p and o/p\n",
    " - all intermediate steps\n",
    " - latency \n",
    " - cost \n",
    " - error \n",
    " - tags \n",
    " - metadata \n",
    " - feedback "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335865a2",
   "metadata": {},
   "source": [
    "## Core Concept\n",
    "1) Project \n",
    "whole project, that is executed mutiple time \n",
    "\n",
    "2) Trace \n",
    "- each time the project is execute it is a trace\n",
    "\n",
    "3) Run\n",
    "- excustion of each trace have mutiple steps, each of the steps is a single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e8e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=\"what is the name of india's first PM?\")\n",
    "chain = prompt | llm_openai | parser\n",
    "result = chain.invoke(input={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9670407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"India's first Prime Minister was Jawaharlal Nehru. He served from August 15, 1947, when India gained independence, until his death on May 27, 1964.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6482bc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **AI's Dual Impact on Employment**: The report highlights that while AI in India can enhance productivity and create new job opportunities in sectors like IT and agriculture, it also poses significant risks of job displacement, particularly in manufacturing, customer service, and other routine task-oriented jobs.\n",
      "\n",
      "2. **Current Employment Landscape**: With over 1.4 billion people in India and a workforce of around 500 million, the unemployment rate is approximately 7-8%, with a substantial portion of employment in the informal sector lacking job security and benefits.\n",
      "\n",
      "3. **Job Displacement Risks**: AI-driven automation is leading to significant job losses, especially for roles involving repetitive tasks, while a mismatch in digital skills among the workforce hampers the transition to new jobs created by AI advancements.\n",
      "\n",
      "4. **Opportunity for Job Creation**: AI is generating demand for new roles such as data scientists and machine learning engineers, and fostering entrepreneurship through innovation, which could lead to job creation if supported by adequate upskilling initiatives.\n",
      "\n",
      "5. **Policy Recommendations**: To address the challenges posed by AI, the report suggests implementing nationwide training programs, fostering public-private partnerships for relevant education, creating social safety nets for displaced workers, and establishing innovation hubs to encourage entrepreneurship in tech sectors.\n"
     ]
    }
   ],
   "source": [
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Generate a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Generate a 5 pointer summary from the following text \\n {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "# This is how one can add project name\n",
    "os.environ['LANGSMITH_PROJECT']= \"Sequential LLM App\"\n",
    "# llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\" , api_key= GOOGLE_API_KEY , temperature=0.9)\n",
    "\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = prompt1 | llm_openai | parser | prompt2 | llm_openai | parser\n",
    "\n",
    "config = RunnableConfig(\n",
    "    run_name=\"sequential_report_generation_v1\",\n",
    "    tags=[\"llm_app\", \"report_generation\"]\n",
    ")\n",
    "result = chain.invoke({'topic': 'Role of Ai in unemplyoment in India'} , config= config)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69dba473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF RAG ready. Ask a question (or Ctrl+C to exit).\n",
      "\n",
      "A: His AWS skills include leveraging serverless AWS architectures to minimize operational costs while ensuring high scalability and security standards.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()  # expects OPENAI_API_KEY in .env\n",
    "\n",
    "PDF_PATH = \"Data\\ShantnuKumar.pdf\"  # <-- change to your PDF filename\n",
    "\n",
    "# 1) Load PDF\n",
    "loader = PyPDFLoader(PDF_PATH)\n",
    "docs = loader.load()  # one Document per page\n",
    "\n",
    "# 2) Chunk\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "splits = splitter.split_documents(docs)\n",
    "\n",
    "# 3) Embed + index\n",
    "vs = FAISS.from_documents(splits, embeddings)\n",
    "retriever = vs.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})\n",
    "\n",
    "# 4) Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer ONLY from the provided context. If not found, say you don't know.\"),\n",
    "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\")\n",
    "])\n",
    "\n",
    "config = RunnableConfig(\n",
    "    run_name=\"rag_example_1\",\n",
    "    tags=[\"llm_app\", \"rag_application\"]\n",
    ")\n",
    "# 5) Chain\n",
    "def format_docs(docs): return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "parallel = RunnableParallel({\n",
    "    \"context\": retriever | RunnableLambda(format_docs),\n",
    "    \"question\": RunnablePassthrough()\n",
    "})\n",
    "\n",
    "chain = parallel | prompt | llm_openai | StrOutputParser()\n",
    "\n",
    "# 6) Ask questions\n",
    "print(\"PDF RAG ready. Ask a question (or Ctrl+C to exit).\")\n",
    "q = input(\"\\nQ: \")\n",
    "ans = chain.invoke(q.strip() ,  config= config)\n",
    "print(\"\\nA:\", ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51d89ff",
   "metadata": {},
   "source": [
    "#### Imp Points\n",
    "- By Default only Runnable are tracked by LangSmith, what about the other part like chunkig, loading etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e477a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF RAG ready. Ask a question (or Ctrl+C to exit).\n",
      "\n",
      "A: He managed the full lifecycle for the SpiceReclaim booking forecast model, which included initial development, optimization, and cloud deployment. He improved the accuracy of Payload Prediction models and transitioned them to production environments, ensuring reliable performance using AWS services such as EC2, Lambda, S3, and EventBridge. He also investigated and resolved critical bugs in the Booking Forecast model under pressure, just days before the SpiceReclaim launch.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langsmith import traceable  # <-- key import\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PDF_PATH = \"Data\\ShantnuKumar.pdf\"  # change to your file\n",
    "\n",
    "@traceable(name=\"load_pdf\")\n",
    "def load_pdf(path: str):\n",
    "    loader = PyPDFLoader(path)\n",
    "    return loader.load()  # list[Document]\n",
    "\n",
    "@traceable(name=\"split_documents\" , metadata={\"text_splitter\" : \"RecursiveCharacterTextSplitter\"})\n",
    "def split_documents(docs, chunk_size=1000, chunk_overlap=150):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "@traceable(name=\"build_vectorstore\" , tags=['vectorDB'] , metadata={\"vector_store\" : \"FAISS\" })\n",
    "def build_vectorstore(splits):\n",
    "    # FAISS.from_documents internally calls the embedding model:\n",
    "    vs = FAISS.from_documents(splits, embeddings)\n",
    "    return vs\n",
    "\n",
    "# You can also trace a “setup” umbrella span if you want:\n",
    "@traceable(name=\"setup_pipeline\")\n",
    "def setup_pipeline(pdf_path: str):\n",
    "    docs = load_pdf(pdf_path)\n",
    "    splits = split_documents(docs)\n",
    "    vs = build_vectorstore(splits)\n",
    "    return vs\n",
    "\n",
    "# ---------- pipeline ----------\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer ONLY from the provided context. If not found, say you don't know.\"),\n",
    "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\")\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "# Build the index under traced setup\n",
    "vectorstore = setup_pipeline(PDF_PATH)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "parallel = RunnableParallel({\n",
    "    \"context\": retriever | RunnableLambda(format_docs),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "})\n",
    "\n",
    "chain = parallel | prompt | llm_openai | StrOutputParser()\n",
    "\n",
    "# ---------- run a query (also traced) ----------\n",
    "print(\"PDF RAG ready. Ask a question (or Ctrl+C to exit).\")\n",
    "q = input(\"\\nQ: \").strip()\n",
    "\n",
    "# Give the visible run name + tags/metadata so it’s easy to find:\n",
    "config = {\n",
    "    \"run_name\": \"pdf_rag_query\"\n",
    "}\n",
    "\n",
    "ans = chain.invoke(q, config=config)\n",
    "print(\"\\nA:\", ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50142c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
